##############################################################################
#
#   Snakemake pipeline:
#   {{cookiecutter.project_name}}
#
#   AUTHOR: Maciej_Bak
#   AFFILIATION: Swiss_Institute_of_Bioinformatics
#   CONTACT: wsciekly.maciek@gmail.com
#   CREATED: 23-09-2019
#   LICENSE: GPL
#
##############################################################################

# imports
import sys
import os

# local rules
localrules: all

##############################################################################
### Target rule with final output of the pipeline
##############################################################################

rule all:
    input:


##############################################################################
### Create directories for the result
##############################################################################

rule create_output_dir:
    output:
        TMP_output = temp("{output_dir}/dir_created")
    params:
        DIR_results_dir = "{output_dir}",
        DIR_cluster_log = "{output_dir}/cluster_log",
    log:
        DIR_local_log = "{output_dir}/local_log",
    shell:
        """
        mkdir -p {params.DIR_results_dir}; \
        mkdir -p {params.DIR_cluster_log}; \
        mkdir -p {log.DIR_local_log}; \
        touch {output.TMP_output}
        """

##############################################################################
### Sample some random data
##############################################################################

rule generate_files:
    input:
        TMP_output = "{output_dir}/dir_created",
        SCRIPT = \
          os.path.join(config["LSM_scripts_directory"],
                       "mb-randomize-matrix.py")
    output:
        TXT_random_sample = "{output_dir}/inclusion_table_randomized.tsv"
    params:
        cluster_log = "{output_dir}/cluster_log/shuffle_inclusion_table.log",
        queue = "30min",
        time = "0:05:00"
    log:
        local_log = "{output_dir}/local_log/shuffle_inclusion_table.log",
    resources:
        threads = 1,
        mem = 5000
    benchmark:
        "{output_dir}/cluster_log/shuffle_inclusion_table_benchmark.log"
    conda:
        "../packages.yaml"
        #"yaml_files/LSM_envs.yaml"
    shell:
        """
        python {input.SCRIPT} \
        --in {input.inclusion_table} \
        --out {output.inclusion_table_out} \
        &> {log.local_log};
        """

################################################################################
# Update results from the model run with
# statistical significance info
################################################################################

rule merge_results:
    input:
        TXT_result_files = expand("{output_dir}/{file}",
            output_dir=config["output_dir"],
            file=config["filenames"])
    output:
        results_updated = "{LSM_output_dir}/{site}/AS_results_updated.tsv"
    params:
        run1_results = "{LSM_output_dir}/{site}/AS_results.tsv",
        cluster_log = os.path.join("{LSM_output_dir}","cluster_log/LSM_update_results_{site}.log"),
        queue = "30min",
        time = "00:05:00"
    resources:
        threads = 1,
        mem = 5000
    log:
        local_log = os.path.join("{LSM_output_dir}","local_log","LSM_update_results_{site}.log")
    run:
        with open(input.AS_results_per_site) as infile:
            significant_motifs_in_regions = [line.split("\t") for line in infile.read().splitlines()[1:]]
            significant_motifs_in_regions = [i[0] for i in significant_motifs_in_regions]
        with open(params.run1_results) as infile, open(output.results_updated,"w") as outfile:
            in_lines = infile.read().splitlines()
            outfile.write(in_lines[0]+"\ttopN_stat_sig\n") #header line
            for line in in_lines[1:]:
                motif_region = line.split("\t")[0]
                if motif_region in significant_motifs_in_regions:
                    outfile.write(line+"\tYES\n")
                else:
                    outfile.write(line+"\tNO\n")